<<<<<<< HEAD
help(deviance)
knitr::opts_chunk$set(echo = TRUE)
RNGversion('3.5.1')
Dataframe=read.csv("australian-crabs.csv")
n = length(Dataframe[,1])
CL = Dataframe$CL
RW = Dataframe$RW
plot(CL, RW, main="Plot of carapace length versus rear width depending on sex",
sub="Red = Female, Blue = Male",
col=c("red", "blue")[Dataframe$sex], xlab="CL", ylab="RW")
#Create function for misclassification rate
missclass=function(conf_matrix, fit_matrix){
n=length(fit_matrix[,1])
return(1-sum(diag(conf_matrix))/n)
}
#LDA analysis with target Sex, and features CL and RW and proportional prior
library("MASS")
model = lda(sex ~ CL+RW, data=Dataframe)
predicted = predict(model, data=Dataframe)
confusion_matrix = table(Dataframe$sex, predicted$class)
misclass = missclass(confusion_matrix, Dataframe)
print(confusion_matrix)
print(misclass)
plot(CL, RW, main="Plot values of CL and RW depending on predicted sex",
sub="Red = Female, Blue = Male",
col=c("red", "blue")[predicted$class], xlab="CL", ylab="RW")
#Repeat step 2 but use priors p(Male)=0.9 and p(Female)=0.1
model2 = lda(sex ~ CL+RW, data=Dataframe, prior=c(1,9)/10)
predicted2 = predict(model2, data=Dataframe)
confusion_matrix2 = table(Dataframe$sex, predicted2$class)
misclass2 = missclass(confusion_matrix2, Dataframe)
print(confusion_matrix2)
print(misclass2)
plot(CL, RW, main="Predicted values of CL and RW with priors 0.9 (male) 0.1 (female)"
, sub="Red = Female, Blue = Male", col=c("red", "blue")[predicted2$class], xlab="CL",
ylab="RW")
#Repeat step 2 but now with logistic regression
model3 = glm(sex ~ CL+RW, data=Dataframe, family='binomial')
predicted3 = predict(model3, newdata=Dataframe, type='response')
sexvector = c()
for (i in predicted3) {
if (i>0.5) {
sexvector = c(sexvector, 'Male')
} else {
sexvector = c(sexvector, 'Female')
}
}
sexvector_factor = as.factor(sexvector)
confusion_matrix3 = table(Dataframe$sex, sexvector_factor)
misclass3 = missclass(confusion_matrix3, Dataframe)
print(confusion_matrix3)
print(misclass3)
plot(CL, RW, main="Predicted values of CL and RW but with logistic regression",
col=c("red", "blue")[sexvector_factor], xlab="CL", ylab="RW", xlim=c(0,50),
ylim=c(0,20))
boundaryline = function(length, coefficientvector, prior) {
return(-coefficientvector[1]/coefficientvector[3]-
(coefficientvector[2]/coefficientvector[3])*length+
log(prior/(1-prior))/coefficientvector[3])
}
par(new=TRUE)
curve(boundaryline(x, model3$coefficients, 0.5), xlab="CL", ylab="RW", col="green",
from=0, to=50, xlim=c(0,50), ylim=c(0,20),
sub="Red = Female, Blue = Male, Green = Boundaryline")
#1: Read data and divide into train, validation and test sets
library("tree")
data=read.csv2("creditscoring.csv")
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.25))
valid=data[id2,]
id3=setdiff(id1,id2)
test=data[id3,]
#Create function for misclassification rate
misclass=function(conf_matrix, fit_matrix){
n=length(fit_matrix[,1])
return(1-sum(diag(conf_matrix))/n)
}
#2: Fit a decision tree to train data using the measures of impurity gini and deviance.
#Report misclass rates and choose optimal measure moving forward.
fit_deviance=tree(good_bad~., data=train, split="deviance")
predicted_deviance=predict(fit_deviance, newdata=test, type="class")
confusionmatrix_deviance=table(test$good_bad, predicted_deviance)
misclass_deviance=misclass(confusionmatrix_deviance, test)
print(confusionmatrix_deviance)
print(misclass_deviance)
fit_gini=tree(good_bad~., data=train, split="gini")
predicted_gini=predict(fit_gini, newdata=test, type="class")
confusionmatrix_gini=table(test$good_bad, predicted_gini)
misclass_gini=misclass(confusionmatrix_gini, test)
print(confusionmatrix_gini)
print(misclass_gini)
#Deviance has best misclass score
#3: Use training and valid data to choose optimal tree depth. Present graphs of the
#dependence of deviances for training and validation data on the number of leaves.
#Report optimal tree, report it's depth and variables used bytree. Estimate
#misclassification rate for the test data.
fit_optimaltree=tree(good_bad~., data=train, split="deviance")
summary(fit_optimaltree)
trainScore=rep(0,15)
testScore=rep(0,15)
for(i in 2:15){
prunedTree=prune.tree(fit_optimaltree, best=i)
pred=predict(prunedTree, newdata=valid, type="tree")
#Divide by two since double of data points
trainScore[i]=deviance(prunedTree)/2
testScore[i]=deviance(pred)
}
plot(2:15, trainScore[2:15], type="b", col="red", ylim=c(200,500))
points(2:15, testScore[2:15], type="b", col="blue")
min_deviance=min(testScore[2:15])
print(min_deviance)
optimal_leaves=which(testScore[1:15] == min_deviance)
print(optimal_leaves)
#Optimal no of leaves is 4
finalTree=prune.tree(fit_optimaltree, best=4)
plot(finalTree)
text(finalTree, pretty=0)
#Final tree contains variables savings, duration and history. Since 3 vars => Depth of
#tree is 3.
predicted_test=predict(finalTree, newdata=test, type="class")
confusionmatrix_test=table(test$good_bad, predicted_test)
misclass_test=misclass(confusionmatrix_test, test)
print(confusionmatrix_test)
print(misclass_test)
#4: Use traning data to perform classification using Naives bayes and report the confusion
#matrices and misclassification rates for the traning and for the test data. Compare with
#results from previous steps.
#Load libraries
library(MASS)
library(e1071)
fit_naive=naiveBayes(good_bad~., data=train)
#Create function for predicting and creating confusion matrice and printing
#misclassification rate
compute_naive=function(model,data){
predictedNaive=predict(model, newdata=data, type="class")
confusionmatrixNaive=table(data$good_bad,predictedNaive)
misclass = misclass(confusionmatrixNaive, data)
print(confusionmatrixNaive)
print(misclass)
return(predictedNaive)
}
predictedNaive_train=compute_naive(fit_naive,train)
predictedNaive_test=compute_naive(fit_naive, test)
#5: Use optimal tree and Naives Bayes to classify the test data by using principle:
#classified as 1 if bigger than 0.05, 0.1, 0.15, ..., 0.9, 0.95. Compute the TPR
#and FPR for two models and plot corresponsing ROC curves.
#Writing function for classifying data
class=function(data, class1, class2, prior){
vector=c()
for(i in data) {
if(i>prior){
vector=c(vector,class1)
} else {
vector=c(vector,class2)
}
}
return(vector)
}
x_vector=seq(0.05,0.95,0.05)
tpr_tree=c()
fpr_tree=c()
tpr_naive=c()
fpr_naive=c()
treeVector=c()
treeConfusion = c()
naiveConfusion = c()
treeClass = c()
naiveClass = c()
#Reusing optimal tree found in task 3 but returntype is response instead
predictTree=data.frame(predict(finalTree, newdata=test, type="vector"))
predictNaive=data.frame(predict(fit_naive, newdata=test, type="raw"))
for(prior in x_vector){
treeClass = class(predictTree$good, 'good', 'bad', prior)
treeConfusion=table(test$good_bad, treeClass)
if(ncol(treeConfusion)==1){
if(colnames(treeConfusion)=="good"){
treeConfusion=cbind(c(0,0), treeConfusion)
} else {
treeConfusion=cbind(treeConfusion,c(0,0))
}
}
totGood=sum(treeConfusion[2,])
totBad=sum(treeConfusion[1,])
tpr_tree=c(tpr_tree, treeConfusion[2,2]/totGood)
fpr_tree=c(fpr_tree, treeConfusion[1,2]/totBad)
naiveClass=class(predictNaive$good, 'good', 'bad', prior)
naiveConfusion=table(test$good_bad, naiveClass)
if(ncol(naiveConfusion)==1){
if(colnames(naiveConfusion)=="good"){
naiveConfusion=cbind(c(0,0), naiveConfusion)
} else {
naiveConfusion=cbind(naiveConfusion,c(0,0))
}
}
totGood=sum(naiveConfusion[2,])
totBad=sum(naiveConfusion[1,])
tpr_naive=c(tpr_naive, naiveConfusion[2,2]/totGood)
fpr_naive=c(fpr_naive, naiveConfusion[1,2]/totBad)
}
#Plot the ROC curves
plot(fpr_naive, tpr_naive, main="ROC curve", sub="Red = Naive Bayes, Blue = Tree",
type="l", col="red", xlim=c(0,1), ylim=c(0,1), xlab="FPR", ylab="TPR")
points(fpr_tree, tpr_tree, type="l", col="blue")
#Naive has greatest AOC => should choose Naive
#6: Repeate Naive Bayes with loss matrix punishing with factor 10 if predicting good when
#bad and 1 if predicting bad when good.
naiveModel=naiveBayes(good_bad~., data=train)
train_loss=predict(naiveModel, newdata=train, type="raw")
test_loss=predict(naiveModel, newdata=test, type="raw")
confusion_trainLoss=table(train$good_bad, ifelse(train_loss[,2]/train_loss[,1]>10, "good",
"bad"))
misclass_trainLoss=misclass(confusion_trainLoss, train)
print(confusion_trainLoss)
print(misclass_trainLoss)
confusion_testLoss=table(test$good_bad, ifelse(test_loss[,2]/test_loss[,1]>10, "good",
"bad"))
misclass_testLoss=misclass(confusion_testLoss, test)
print(confusion_testLoss)
print(misclass_testLoss)
#1: Read data
data=read.csv2("NIRspectra.csv")
data$Viscosity=c()
n=dim(data)[1]
#1: Conduct standard PCA using the feature space and provide a plot explaining how much
#variation is explained by each feature. Provide plot that show the scores of PC1 vs PC2.
#Are there unusual diesel fuels according to this plot.
pcaAnalysis=prcomp(data)
#Eigenvalues
lambda=pcaAnalysis$sdev^2
#Proportion of variation
propVar= lambda/sum(lambda)*100
screeplot(pcaAnalysis, main="Total variation from PCA components")
noOfVars=1
sumOfVariation=propVar[noOfVars]
while(sumOfVariation<99){
noOfVars=noOfVars+1
sumOfVariation=sumOfVariation+propVar[noOfVars]
}
#Print number of variables used and total variation
print(noOfVars)
print(sumOfVariation)
#Print PC1 and PC2 in plot
plot(pcaAnalysis$x[,1],pcaAnalysis$x[,2], type="p", col="blue", main="PC1 vs PC2",
xlab="PC1", ylab="PC2")
#We can see from the graph that the data is very accurately described by PC1.
#2: Make trace plots of the loadings of the components selected in step 1. Is there any
#principle component that is explaines by mainly a few original features?
U=pcaAnalysis$rotation
plot(U[,1], main="Traceplot, PC1", xlab="index", ylab="PC1", type="b")
plot(U[,2], main="Traceplot, PC2", xlab="index", ylab="PC2", type="b")
#We can see from graph that PC2 is not described by so many original features since it is
#close to zero for many of the features. The last 30 or so variables have an effect on PC2.
#3: Perform independent Component Analysis (ICA) with no of components selected in step1
#(set seed 12345). Check the documentation of R for fastICA method and do following:
# Compute W'=K*W and present columns of W' in form of the trace plots. Compare with trace
# plots in step 2 and make conclusions. What kind of measure is represented by the matrix W'.
# Make a plot of the scores of the first two latent features and compare it with the score
# plot from step 1.
#Install package fastICa
#install.packages("fastICA")
library("fastICA")
set.seed(12345)
icaModel = fastICA(data, n.comp=2, verbose=TRUE)
W=icaModel$W
K=icaModel$K
W_est=K%*%W
plot(W_est[,1], main="Traceplot, ICA1", xlab="index", ylab="ICA1", type="b", col="red")
plot(W_est[,2], main="Traceplot, ICA2", xlab="index", ylab="ICA2", type="b", col="red")
#Compared to the plots in step 2 the ICA1 follows in roughly the same pattern as PCA2
#and ICA2 the same as PCA1.
plot(icaModel$S[,1], icaModel$S[,2], main="ICA1 vs ICA2", xlab="ICA1", ylab="ICA2",
type="p", col="blue")
#We can see from the plot that the dat is pretty well described by ICA2 whereas ICA1 is
#not that significant in describing the data (since it is close to 0 most of the cases).
#Some outliers are however described by ICA1.
setwd("~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1")
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
G=sort(G)
G=G[0.05*10000:10000]
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
## Assignment 2: Assume that you have asked 10 randomly selected persons about their monthly
## income(inthousandsSwedishKrona)andobtainedthefollowingtenobservations: 44, 25, 45, 52, 30, 63, 19, 50, 34
## and 67. A common model for non-negative continuous variables is the log-normal distribution. The log-normal
## distribution log(N(my, sigma^2)) has density function ... for y > 0, my > 0 and sigma > 0. The log-normal
## distribution is related to the normal distribution as follows: if y ~ log N(my, sigma^2) then
## log y ~ N(my, sigma^2). Let y1,...,yn given my and simga^2 ~ log N(my, sigma^2), where my=3.7 is assumed to be
## known but sigma^2 is unknown with noninformative prior p(sigma^2) is proportional to 1/sigma^2. The posterior
## for sigma^2 is the Inv - chitwo distribution with X(n, thao^2) distribution, where thao^2=sum((log(yi)-my)^2)/n
## a) Simulate 10 000 draws from the posterior of sigma^2 (assuming my=3.7) and compare with the theoretical
## with the theoretical Inv - chitwo distribution with X(n, thao^2) posterior distribution.
library(geoR)
x=c(44, 25, 45, 52, 30, 63, 19, 50, 34, 67)
n=length(x)
my=3.7
#Function for calculating thao^2
calcThao = function(data, my, n) {
return(sum((log(data)-my)^2)/n)
}
thaosq=calcThao(x, my, n)
set.seed(12345)
drawX=rchisq(10000, n)
sigmasq=(n)*thaosq/drawX
xvals=seq(0.001, 3, 0.001)
plot(density(sigmasq), main="Density of simulated sigma^2, black = simulated distrib., red = actual distrib.")
lines(xvals,dinvchisq(xvals, n, thaosq), col="red")
## As seen in the plot the theoretical distribution (red line) follows the simulated one with good precision. This
## indicates that the simulation has been made correctly.
## b) The most common measure of income inequality is the Gini coefficient, G, where 0<=G<=1. G=0 means a
## completely equal income distribution, whereas G=1 means complete income inequality. See Wikipedia for more
## information. It can be shown that G=2*CDF-normal(sigma/sqrt(2))-1 when income follow a log N(my, sigma^2)
## distribution.  Use the posterior draws in a) to compute the posterior distribution of the Gini coefficient G
## for the current data set.
G=2*pnorm(sqrt(sigmasq/2), mean=0, sd=1)-1
hist(G, breaks=100)
plot(density(G))
## As seen in the plot the gini coefficient is centered at around 0.2 which means a rather inequal distribution.
## Use the posterior draws from b) to compute a 90% equal tail credible interval for G. A 90% equal tail interval
## (a,b) cuts off 5% percent of the posterior probability mass to the left of a, and 5% to the right of b. Also,
## do a kernel density estimate of the posterior of G using the density function in R with defaultsettings,
## and use that kernel density estimate to compute a 90% Highest Posterior Density interval for G. Compare the
## two intervals.
G=sort(G)
G=G[-(1:0.05*length(G)),1]
G=G[-(1:0.05*length(G))]
## Assignment 2: Assume that you have asked 10 randomly selected persons about their monthly
## income(inthousandsSwedishKrona)andobtainedthefollowingtenobservations: 44, 25, 45, 52, 30, 63, 19, 50, 34
## and 67. A common model for non-negative continuous variables is the log-normal distribution. The log-normal
## distribution log(N(my, sigma^2)) has density function ... for y > 0, my > 0 and sigma > 0. The log-normal
## distribution is related to the normal distribution as follows: if y ~ log N(my, sigma^2) then
## log y ~ N(my, sigma^2). Let y1,...,yn given my and simga^2 ~ log N(my, sigma^2), where my=3.7 is assumed to be
## known but sigma^2 is unknown with noninformative prior p(sigma^2) is proportional to 1/sigma^2. The posterior
## for sigma^2 is the Inv - chitwo distribution with X(n, thao^2) distribution, where thao^2=sum((log(yi)-my)^2)/n
## a) Simulate 10 000 draws from the posterior of sigma^2 (assuming my=3.7) and compare with the theoretical
## with the theoretical Inv - chitwo distribution with X(n, thao^2) posterior distribution.
library(geoR)
x=c(44, 25, 45, 52, 30, 63, 19, 50, 34, 67)
n=length(x)
my=3.7
#Function for calculating thao^2
calcThao = function(data, my, n) {
return(sum((log(data)-my)^2)/n)
}
thaosq=calcThao(x, my, n)
set.seed(12345)
drawX=rchisq(10000, n)
sigmasq=(n)*thaosq/drawX
xvals=seq(0.001, 3, 0.001)
plot(density(sigmasq), main="Density of simulated sigma^2, black = simulated distrib., red = actual distrib.")
lines(xvals,dinvchisq(xvals, n, thaosq), col="red")
## As seen in the plot the theoretical distribution (red line) follows the simulated one with good precision. This
## indicates that the simulation has been made correctly.
## b) The most common measure of income inequality is the Gini coefficient, G, where 0<=G<=1. G=0 means a
## completely equal income distribution, whereas G=1 means complete income inequality. See Wikipedia for more
## information. It can be shown that G=2*CDF-normal(sigma/sqrt(2))-1 when income follow a log N(my, sigma^2)
## distribution.  Use the posterior draws in a) to compute the posterior distribution of the Gini coefficient G
## for the current data set.
G=2*pnorm(sqrt(sigmasq/2), mean=0, sd=1)-1
hist(G, breaks=100)
plot(density(G))
## As seen in the plot the gini coefficient is centered at around 0.2 which means a rather inequal distribution.
## Use the posterior draws from b) to compute a 90% equal tail credible interval for G. A 90% equal tail interval
## (a,b) cuts off 5% percent of the posterior probability mass to the left of a, and 5% to the right of b. Also,
## do a kernel density estimate of the posterior of G using the density function in R with defaultsettings,
## and use that kernel density estimate to compute a 90% Highest Posterior Density interval for G. Compare the
## two intervals.
G=sort(G)
G=G[-(1:(0.05*length(G)))]
G=G[-(0.95*length(G):length(G))]
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
help(density)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
help(abline)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
help(abline)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
setwd("~/SKOLA/LIU/Åk 4/TDDE01/TDDE01_Labs/Lab2 Block 1")
source('~/SKOLA/LIU/Åk 4/TDDE01/TDDE01_Labs/Lab2 Block 1/Lab2_Assignment3.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
GDensity=density(G)
View(GDensity)
help(density)
View(GDensity)
GDensitySorted=sort(GDensity$y)
View(GDensity)
help("density")
gDensity$y
GDensity$y
help(with)
plot(GDensity)
View(GDensity)
GDensity$x
GDensity$y
help(sum)
help(cumsum)
GDensity.df=data.frame(GDensity$x, GDensity$y)
View(GDensity.df)
CumDensity=cumsum(GDensity$y)/sum(GDensity$y)
cumDensity
CumDensity
help(which)
GDensity_CredInterval=GDensity.df[GDensity.df$y>0.05,]
View(GDensity_CredInterval)
GDensity.df$y=cumsum(GDensity$y)/sum(GDensity$y)
View(GDensity.df)
GDensity=density(G)
GDensity.df=data.frame(x=GDensity$x, y=GDensity$y)
GDensity.df$y=cumsum(GDensity$y)/sum(GDensity$y)
View(GDensity.df)
GDensity_CredInterval=GDensity.df[GDensity.df$y>0.05,]
View(GDensity_CredInterval)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
View(GDensity_CredInterval_Vals)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
View(GDensity.df)
View(GDensity_CredInterval_Vals)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
View(GDensity_CredInterval_Vals)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
help(sub)
help(subtitle)
source('~/SKOLA/LIU/Åk 4/TDDE07/TDDE07_Labs/Lab1/Lab1_Assignment2.R', echo=TRUE)
=======
res=boot(Dataframe, statistic=f1, R=1000, mle=linear_model, ran.gen=rng, sim="parametric")
e=envelope(res)
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,200))
points(seq(12,13,0.01), e$point[2,], type="l", lty=21, col="grey")
source('~/SKOLA/LIU/Åk 4/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
help(boot)
View(res)
library(boot)
rng=function(data, mle) {
data1=data.frame(Visitors=numeric(21), Time=data)
n=length(data1$Visitors)
#generate new Price
data1$Visitors=rnorm(n,predict(mle, newdata=data1),sd(mle$residuals))
return(data1)
}
f1=function(data1){
res=lm(Visitors~., data=data1) #fit linearmodel
#predictvaluesfor all Visitor valuesfrom the original data
Visitors=predict(res,newdata=Dataframe)
n=length(data1$Visitors)
predictedVisitors=rnorm(n, Visitors, sd(linear_model$residuals))
print(predictedVisitors)
return(predictedVisitors)
}
res=boot(seq(12,13,0.05), statistic=f1, R=1000, mle=linear_model, ran.gen=rng, sim="parametric")
e=envelope(res)
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,200))
points(seq(12,13,0.01), e$point[2,], type="l", lty=21, col="grey")
library(boot)
rng=function(data, mle) {
data1=data.frame(Visitors=data$Visitors, Time=data$Time)
n=length(seq(12,13,0.05))
#generate new Price
data1$Visitors=rnorm(n,predict(mle, newdata=seq(12,13,0.05)),sd(mle$residuals))
return(data1)
}
f1=function(data1){
res=lm(Visitors~., data=data1) #fit linearmodel
#predictvaluesfor all Visitor valuesfrom the original data
Visitors=predict(res,newdata=Dataframe)
n=length(data1$Visitors)
predictedVisitors=rnorm(n, Visitors, sd(linear_model$residuals))
return(predictedVisitors)
}
res=boot(Dataframe, statistic=f1, R=1000, mle=linear_model, ran.gen=rng, sim="parametric")
e=envelope(res)
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,200))
points(seq(12,13,0.01), e$point[2,], type="l", lty=21, col="grey")
library(boot)
rng=function(data, mle) {
data1=data.frame(Visitors=data$Visitors, Time=data$Time)
n=length(seq(12,13,0.05))
#generate new Price
data1$Visitors=rnorm(n,predict(mle, newdata=as.data.frame(seq(12,13,0.05))), sd(mle$residuals))
return(data1)
}
f1=function(data1){
res=lm(Visitors~., data=data1) #fit linearmodel
#predictvaluesfor all Visitor valuesfrom the original data
Visitors=predict(res,newdata=Dataframe)
n=length(data1$Visitors)
predictedVisitors=rnorm(n, Visitors, sd(linear_model$residuals))
return(predictedVisitors)
}
res=boot(Dataframe, statistic=f1, R=1000, mle=linear_model, ran.gen=rng, sim="parametric")
e=envelope(res)
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,200))
points(seq(12,13,0.01), e$point[2,], type="l", lty=21, col="grey")
View(Dataframe)
as.data.frame(Time=seq(1,2,0.1))
length(seq(12,13,0.05))
as.matrix(seq(12,13,0.05))
library(boot)
rng=function(data, mle) {
data1=data.frame(Visitors=data$Visitors, Time=data$Time)
n=length(seq(12,13,0.05))
#generate new Price
data1$Visitors=rnorm(n,predict(mle, newdata=as.matrix(seq(12,13,0.05))), sd(mle$residuals))
return(data1)
}
f1=function(data1){
res=lm(Visitors~., data=data1) #fit linearmodel
#predictvaluesfor all Visitor valuesfrom the original data
Visitors=predict(res,newdata=Dataframe)
n=length(data1$Visitors)
predictedVisitors=rnorm(n, Visitors, sd(linear_model$residuals))
return(predictedVisitors)
}
res=boot(Dataframe, statistic=f1, R=1000, mle=linear_model, ran.gen=rng, sim="parametric")
e=envelope(res)
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,200))
points(seq(12,13,0.01), e$point[2,], type="l", lty=21, col="grey")
library(boot)
rng=function(data, mle) {
data1=data.frame(Visitors=data$Visitors, Time=data$Time)
n=length(seq(12,13,0.05))
#generate new Price
data1$Visitors=rnorm(n,predict(mle, newdata=data.frame(Time=seq(12,13,0.05))), sd(mle$residuals))
return(data1)
}
f1=function(data1){
res=lm(Visitors~., data=data1) #fit linearmodel
#predictvaluesfor all Visitor valuesfrom the original data
Visitors=predict(res,newdata=Dataframe)
n=length(data1$Visitors)
predictedVisitors=rnorm(n, Visitors, sd(linear_model$residuals))
return(predictedVisitors)
}
res=boot(Dataframe, statistic=f1, R=1000, mle=linear_model, ran.gen=rng, sim="parametric")
e=envelope(res)
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,200))
points(seq(12,13,0.01), e$point[2,], type="l", lty=21, col="grey")
library(boot)
rng=function(data, mle) {
data1=data.frame(Visitors=data$Visitors, Time=data$Time)
n=length(data$Visitors)
#generate new Price
data1$Visitors=rnorm(n,predict(mle, newdata=data1), sd(mle$residuals))
return(data1)
}
f1=function(data1){
res=lm(Visitors~., data=data1) #fit linearmodel
#predictvaluesfor all Visitor valuesfrom the original data
Visitors=predict(res,newdata=data.frame(Time=seq(12,13,0.05)))
n=21
predictedVisitors=rnorm(n, Visitors, sd(linear_model$residuals))
return(predictedVisitors)
}
res=boot(Dataframe, statistic=f1, R=1000, mle=linear_model, ran.gen=rng, sim="parametric")
e=envelope(res)
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,200))
points(seq(12,13,0.01), e$point[2,], type="l", lty=21, col="grey")
View(res)
View(e)
data.frame(Time=seq(12,13,0.05))
library(boot)
rng=function(data, mle) {
data1=data.frame(Visitors=data$Visitors, Time=data$Time)
n=length(data$Visitors)
#generate new Price
data1$Visitors=rnorm(n,predict(mle, newdata=data1), sd(mle$residuals))
return(data1)
}
f1=function(data1){
res=lm(Visitors~., data=data1) #fit linearmodel
#predictvaluesfor all Visitor valuesfrom the original data
Visitors=predict(res,newdata=data.frame(Time=seq(12,13,0.05)))
n=21
predictedVisitors=rnorm(n, Visitors, sd(linear_model$residuals))
return(predictedVisitors)
}
res=boot(Dataframe, statistic=f1, R=1000, mle=linear_model, ran.gen=rng, sim="parametric")
e=envelope(res)
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,200))
points(seq(12,13,0.05), e$point[2,], type="l", lty=21, col="grey")
help(points)
help(curve)
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,200))
points(seq(12,13,0.05), e$point[2,], type="l", lty=21, col="blue")
points(seq(12,13,0.05), e$point[1,], type="l", lty=21, col="blue")
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,500))
points(seq(12,13,0.05), e$point[2,], type="l", lty=21, col="blue")
points(seq(12,13,0.05), e$point[1,], type="l", lty=21, col="blue")
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,500))
points(seq(12,13,0.05), e$point[2,], type="l", col="blue")
points(seq(12,13,0.05), e$point[1,], type="l", col="blue")
source('~/SKOLA/LIU/Åk 4/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
plot(seq(12,13,0.05), e$point[2,], type="l", col="blue")
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,500))
plot(seq(12,13,0.05), e$point[2,], type="l", col="blue")
points(seq(12,13,0.05), e$point[1,], type="l", col="blue")
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,500))
points(seq(12,13,0.05), e$point[2,], type="l", col="blue")
points(seq(12,13,0.05), e$point[1,], type="l", col="blue")
View(e)
source('~/SKOLA/LIU/Åk 4/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
View(e)
source('~/SKOLA/LIU/Åk 4/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,500))
points(seq(12,13,0.05), exp(e$point[2,]), type="l", col="gray")
points(seq(12,13,0.05), exp(e$point[1,]), type="l", col="gray")
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,500))
points(seq(12,13,0.05), exp(e$point[2,]), type="l", lty=21, col="gray")
points(seq(12,13,0.05), exp(e$point[1,]), type="l", lty=21, col="gray")
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,500))
points(seq(12,13,0.05), exp(e$point[2,]), type="l", lty=12, col="gray")
points(seq(12,13,0.05), exp(e$point[1,]), type="l", lty=12, col="gray")
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,500))
points(seq(12,13,0.05), exp(e$point[2,]), type="l", lty=21, col="gray")
points(seq(12,13,0.05), exp(e$point[1,]), type="l", lty=21, col="gray")
library(boot)
rng=function(data, mle) {
data1=data.frame(Visitors=data$Visitors, Time=data$Time)
n=length(data$Visitors)
#generate new Price
data1$Visitors=rnorm(n,predict(mle, newdata=data1), sd(mle$residuals))
return(data1)
}
f1=function(data1){
res=lm(Visitors~., data=data1) #fit linearmodel
#predictvaluesfor all Visitor valuesfrom the original data
Visitors=predict(res,newdata=data.frame(Time=seq(12,13,0.05)))
n=length(seq(12,13,0.05))
predictedVisitors=rnorm(n, Visitors, sd(linear_model$residuals))
return(predictedVisitors)
}
res=boot(Dataframe, statistic=f1, R=1000, mle=linear_model, ran.gen=rng, sim="parametric")
e=envelope(res)
plot(Dataframe$Time, Dataframe$Visitors, main="Forecasting of visitors depending on time", xlab="Time",
ylab="Visitors", xlim=c(9,13), ylim=c(30,500))
points(seq(12,13,0.05), exp(e$point[2,]), type="l", lty=21, col="gray")
points(seq(12,13,0.05), exp(e$point[1,]), type="l", lty=21, col="gray")
min_value_13=exp(e$point[2,21])
max_value_13=exp(e$point[1,21])
help("cat)
help(cat)
help("cat")
cat("The bank should expect between", min_value_13, "and", max_value_13, "customers", sep=" ")
source('~/SKOLA/LIU/Åk 4/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
help(envelope)
library(boot)
help(envelope)
setwd("~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18")
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
plot(res)
seq(12,13,0.05)
View(Dataframe)
library(neuralnet)
help(neuralnet)
setwd("~/Documents/TDDE01/TDDE01_Labs/Exam/2016-01-09")
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2016-01-09/2016-01-09_Assignment2.R', echo=TRUE)
svmmodel=ksvm(type~., data=fold1, kernel="rbfdot", C=1, cross=2)
View(svmmodel)
cross(svmmodel)
svmmodel=ksvm(type~., data=fold1, kernel="rbfdot", C=1, cross=10)
cross(ksvm)
cross(svmmodel)
svmmodel=ksvm(type~., data=fold1, kernel="rbfdot", C=1)
View(svmmodel)
View(svmmodel)
svmmodel=ksvm(type~., data=fold2, kernel="rbfdot", C=1)
View(svmmodel)
svmmodel=ksvm(type~., data=fold2, kernel="rbfdot", C=1, cross=10)
View(svmmodel)
View(svmmodel)
svmmodel=ksvm(type~., data=fold2, kernel="rbfdot", C=1)
View(svmmodel)
svmmodel=ksvm(type~., data=fold2, kernel="rbfdot", C=1, cross=5)
View(svmmodel)
svmmodel=ksvm(type~., data=fold2, kernel="rbfdot", C=1, cross=10)
View(svmmodel)
View(svmmodel)
svmmodel=ksvm(type~., data=fold2, kernel="rbfdot", C=1, cross=5)
View(svmmodel)
svmmodel=ksvm(type~., data=fold1, kernel="rbfdot", C=1, cross=2)
View(svmmodel)
svmmodel=ksvm(type~., data=fold1, kernel="rbfdot", C=1)
View(svmmodel)
predict(ksvm, newdata=fold1, type="response")
y=predict(svmmodel, newdata=fold1, type="response")
View(fold1)
y
m=table(fold1$type, y)
missclass(m, y)
missclass(m, fold1)
svmmodel=ksvm(type~., data=fold1, kernel="rbfdot", C=100)
View(svmmodel)
View(svmmodel)
svmmodel=ksvm(type~., data=fold1, kernel="rbfdot", C=10)
View(svmmodel)
svmmodel=ksvm(type~., data=fold1, kernel="rbfdot", C=1, cross=100)
View(svmmodel)
svmmodel=ksvm(type~., data=fold1, kernel="rbfdot", C=1, cross=10)
View(svmmodel)
svmmodel=ksvm(type~., data=fold1, kernel="rbfdot", C=1, cross=3)
View(svmmodel)
svmmodel=ksvm(type~., data=fold1, kernel="rbfdot", C=1, cross=500)
View(svmmodel)
help(glm)
help(factor)
setwd("~/Documents/TDDE01/TDDE01_Labs/Exam/2019-01-16")
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2019-01-16/2019-01-16_Assignment2.R', echo=TRUE)
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 3)
tr <- data.frame(Var, Sin=sin(Var))
Var <- runif(50, 3, 9)
te <- data.frame(Var, Sin=sin(Var))
n = dim(tr)[1]
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(10, -1, 1)
nn=neuralnet(Sin~Var, data=tr, hidden=3, startweights=winit)
plot(nn)
pred=predict(nn, newdata=te)
plot(tr$Var, tr$Sin, xlim=c(0,9), ylim=c(-2,2), xlab="Var", ylab="Sin")
points(te$Var, te$Sin, col="blue")
points(te$Var, pred, col="red")
##Answer: The plot resembles the one given so it is confirmed.
##In the previous figure, it is not surprising the poor performance on the range [3,9] because no training point falls
##in that interval. However, it seems that the predictions converge to -2 as the value of Var increases. Why do they
##converge to that particular value ? To answer this question, you may want to look into the weights of the NN
##learned.
plot(nn)
help(tree'')
help(tree)
library(tree)
help(tree)
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 3)
tr <- data.frame(Var, Sin=sin(Var))
Var <- runif(50, 3, 9)
te <- data.frame(Var, Sin=sin(Var))
n = dim(tr)[1]
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(10, -1, 1)
nn=neuralnet(Sin~Var, data=tr, hidden=3, startweights=winit)
plot(nn)
pred=predict(nn, newdata=te)
plot(tr$Var, tr$Sin, xlim=c(0,9), ylim=c(-2,2), xlab="Var", ylab="Sin")
points(te$Var, te$Sin, col="blue")
points(te$Var, pred, col="red")
help("neuralnet")
help(par)
help(points)
help(plot)
plot(train$Var, train$Sin, col = 'black', ylim = c(-2,2), xlim = c(0,9))
par(new = TRUE)
plot(test$Var, test$Sin, col = 'blue', xlab="", ylab="", axes=FALSE, ylim = c(-2,2), xlim = c(0,9))
par(new = TRUE)
plot(test$Var, predictTest, col = 'red', xlab="", ylab="", axes=FALSE, ylim = c(-2,2), xlim = c(0,9))
plot(train$Var, train$Sin, col = 'black', ylim = c(-2,2), xlim = c(0,9))
par(new = TRUE)
plot(test$Var, test$Sin, col = 'blue', xlab="", ylab="", axes=FALSE, ylim = c(-2,2), xlim = c(0,9))
par(new = TRUE)
plot(test$Var, pred, col = 'red', xlab="", ylab="", axes=FALSE, ylim = c(-2,2), xlim = c(0,9))
plot(tr$Var, tr$Sin, col = 'black', ylim = c(-2,2), xlim = c(0,9))
par(new = TRUE)
plot(te$Var, te$Sin, col = 'blue', xlab="", ylab="", axes=FALSE, ylim = c(-2,2), xlim = c(0,9))
par(new = TRUE)
plot(te$Var, pred, col = 'red', xlab="", ylab="", axes=FALSE, ylim = c(-2,2), xlim = c(0,9))
plot(tr$Var, tr$Sin, col = 'black', ylim = c(-2,2), xlim = c(0,9))
par(new = TRUE)
plot(te$Var, te$Sin, col = 'blue', xlab="", ylab="", axes=FALSE, ylim = c(-2,2), xlim = c(0,9))
par(new = TRUE)
plot(te$Var, pred, col = 'red', xlab="", ylab="", axes=FALSE, ylim = c(-2,2), xlim = c(0,9))
plot(tr$Var, tr$Sin, col = 'black', ylim = c(-2,2), xlim = c(0,9))
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2019-01-16/2019-01-16_Assignment2.R', echo=TRUE)
setwd("~/Documents/TDDE01/TDDE01_Labs/Lab2 Block 1")
source('~/Documents/TDDE01/TDDE01_Labs/Lab2 Block 1/Lab2_Assignment3.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Lab2 Block 1/Lab2_Assignment3.R', echo=TRUE)
plot(MET, EX, xlab="EX", ylab="MET", pch=21, bg="orange", main="Plot original vs predicted data", ylim=c(100,500))
points(MET, predData, type="l", col="blue")
points(MET, confIntNPBoot$point[2,], type="l")
points(MET, confIntNPBoot$point[1,], type="l")
source('~/Documents/TDDE01/TDDE01_Labs/Lab2 Block 1/Lab2_Assignment3.R', echo=TRUE)
mle=prune.tree(treemodel, best=3)
summaryMLE = summary(mle)
rng=function(data, mle) {
data1=data.frame(EX=data$EX, MET=data$MET)
n=length(data$EX)
#generatenew EX
data1$EX=rnorm(n,predict(mle, newdata=data1), sd(summaryMLE$residuals))
return(data1)
}
f1=function(data1){
treemodel=tree(EX~MET, data=data1, control=tree.control(48,mincut=8)) #fit linearmodel
prunedtree=prune.tree(treemodel, best=3)
n=length(Dataframe$EX)
#predictvaluesfor all EX values from the original data
predData=predict(prunedtree,newdata=Dataframe)
predictedEX=rnorm(n, predData, sd(summaryMLE$residuals))
return(predictedEX)
}
res=boot(Dataframe, statistic=f1, R=1000, mle=mle, ran.gen=rng, sim="parametric")
predIntPBoot=envelope(res)
points(MET, predIntPBoot$point[2,], type="l", col="green")
points(MET, predIntPBoot$point[1,], type="l", col="green")
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2019-01-16/2019-01-16_Assignment2.R', echo=TRUE)
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 3)
tr <- data.frame(Var, Sin=sin(Var))
Var <- runif(50, 3, 9)
te <- data.frame(Var, Sin=sin(Var))
n = dim(tr)[1]
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(10, -1, 1)
nn=neuralnet(Sin~Var, data=tr, hidden=3, startweights=winit)
plot(nn)
pred=predict(nn, newdata=te)
plot(tr$Var, tr$Sin, xlim=c(0,9), ylim=c(-2,2), xlab="Var", ylab="Sin")
points(te$Var, te$Sin, col="blue")
points(te$Var, pred, col="red")
plot(nn)
setwd("~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18")
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment1.R', echo=TRUE)
install.packages(e1071)
install.packages("e1071")
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment1.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment1.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Lab2 Block 1/Lab2_Assignment3.R', echo=TRUE)
setwd("~/Documents/TDDE01/TDDE01_Labs/Lab2 Block 1")
source('~/Documents/TDDE01/TDDE01_Labs/Lab2 Block 1/Lab2_Assignment3.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment1.R', echo=TRUE)
View(Dataframe)
mean(predPCA != Dataframe$species)
misclass_naivePCA=misclass(confusion_naivePCA, Dataframe)
misclass(confusion_naivePCA, Dataframe)
help(naiveBayes)
response
View(naiveData)
help("predict.naiveBayes")
View(Dataframe)
print(confusion_naivePCA)
Dataframe$response
Dataframe$species
View(naiveData)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2019-01-16/2019-01-16_Assignment2.R', echo=TRUE)
setwd("~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18")
f1=function(data1){
res=lm(Visitors~., data=data1) #fit linearmodel
#predictvaluesfor all Visitor valuesfrom the original data
Visitors=predict(res,newdata=data.frame(Time=seq(12,13,0.05)))
n=length(seq(12,13,0.05))
predictedVisitors=rnorm(n, Visitors, sd(linear_model$residuals))
return(predictedVisitors)
}
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
warnings()
help("predict.glm")
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
View(res)
View(e)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
View(res)
help("predict.glm")
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
predict(linear_model, newdata=Dataframe, type="response")
View(linear_model)
help("predict.glm")
View(res)
View(res)
library(boot)
rng=function(data, mle) {
data1=data.frame(Visitors=data$Visitors, Time=data$Time)
n=length(data$Visitors)
#generate new Price
data1$Visitors=rnorm(n,predict(mle, newdata=data1, type="response"), sd(mle$residuals))
return(data1)
}
f1=function(data1){
res=glm(Visitors~., data=data1, family="poisson") #fit linearmodel
#predictvaluesfor all Visitor valuesfrom the original data
Visitors=predict(res,newdata=data.frame(Time=seq(12,13,0.05)), type="response")
n=length(seq(12,13,0.05))
predictedVisitors=rnorm(n, Visitors, sd(linear_model$residuals))
return(predictedVisitors)
}
res=boot(Dataframe, statistic=f1, R=1000, mle=linear_model, ran.gen=rng, sim="parametric")
warnings()
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
View(e)
View(e)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
View(e)
e[["point"]]
help("predict.glm")
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
View(e)
View(e)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
View(e)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
predict(linear_model, newdata=Dataframe, type="response")
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
View(e)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment1.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Exam/2017-04-18/2017-04-18_Assignment2.R', echo=TRUE)
setwd("~/Documents/TDDE01/TDDE01_Labs/Lab2 Block 1")
source('~/Documents/TDDE01/TDDE01_Labs/Lab2 Block 1/Lab2_Assignment3.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Lab2 Block 1/Lab2_Assignment3.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Lab2 Block 1/Lab2_Assignment3.R', echo=TRUE)
source('~/Documents/TDDE01/TDDE01_Labs/Lab2 Block 1/Lab2_Assignment3.R', echo=TRUE)
source('~/Downloads/SpecialTask2019Backpropagation.R', echo=TRUE)
>>>>>>> 76477b92e208e92338921dd12bed7dd60fba4c6d
