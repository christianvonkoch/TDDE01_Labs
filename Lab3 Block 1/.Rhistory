print(misclass_deviance)
fit_gini=tree(good_bad~., data=train, split="gini")
predicted_gini=predict(fit_gini, newdata=test, type="class")
confusionmatrix_gini=table(test$good_bad, predicted_gini)
misclass_gini=misclass(confusionmatrix_gini, test)
print(confusionmatrix_gini)
print(misclass_gini)
#Deviance has best misclass score
#3: Use training and valid data to choose optimal tree depth. Present graphs of the
#dependence of deviances for training and validation data on the number of leaves.
#Report optimal tree, report it's depth and variables used bytree. Estimate
#misclassification rate for the test data.
fit_optimaltree=tree(good_bad~., data=train, split="deviance")
summary(fit_optimaltree)
trainScore=rep(0,15)
testScore=rep(0,15)
for(i in 2:15){
prunedTree=prune.tree(fit_optimaltree, best=i)
pred=predict(prunedTree, newdata=valid, type="tree")
#Divide by two since double of data points
trainScore[i]=deviance(prunedTree)/2
testScore[i]=deviance(pred)
}
plot(2:15, trainScore[2:15], type="b", col="red", ylim=c(200,500))
points(2:15, testScore[2:15], type="b", col="blue")
min_deviance=min(testScore[2:15])
print(min_deviance)
optimal_leaves=which(testScore[1:15] == min_deviance)
print(optimal_leaves)
#Optimal no of leaves is 4
finalTree=prune.tree(fit_optimaltree, best=4)
plot(finalTree)
text(finalTree, pretty=0)
#Final tree contains variables savings, duration and history. Since 3 vars => Depth of
#tree is 3.
predicted_test=predict(finalTree, newdata=test, type="class")
confusionmatrix_test=table(test$good_bad, predicted_test)
misclass_test=misclass(confusionmatrix_test, test)
print(confusionmatrix_test)
print(misclass_test)
#4: Use traning data to perform classification using Naives bayes and report the confusion
#matrices and misclassification rates for the traning and for the test data. Compare with
#results from previous steps.
#Load libraries
library(MASS)
library(e1071)
fit_naive=naiveBayes(good_bad~., data=train)
#Create function for predicting and creating confusion matrice and printing
#misclassification rate
compute_naive=function(model,data){
predictedNaive=predict(model, newdata=data, type="class")
confusionmatrixNaive=table(data$good_bad,predictedNaive)
misclass = misclass(confusionmatrixNaive, data)
print(confusionmatrixNaive)
print(misclass)
return(predictedNaive)
}
predictedNaive_train=compute_naive(fit_naive,train)
predictedNaive_test=compute_naive(fit_naive, test)
#5: Use optimal tree and Naives Bayes to classify the test data by using principle:
#classified as 1 if bigger than 0.05, 0.1, 0.15, ..., 0.9, 0.95. Compute the TPR
#and FPR for two models and plot corresponsing ROC curves.
#Writing function for classifying data
class=function(data, class1, class2, prior){
vector=c()
for(i in data) {
if(i>prior){
vector=c(vector,class1)
} else {
vector=c(vector,class2)
}
}
return(vector)
}
x_vector=seq(0.05,0.95,0.05)
tpr_tree=c()
fpr_tree=c()
tpr_naive=c()
fpr_naive=c()
treeVector=c()
treeConfusion = c()
naiveConfusion = c()
treeClass = c()
naiveClass = c()
#Reusing optimal tree found in task 3 but returntype is response instead
predictTree=data.frame(predict(finalTree, newdata=test, type="vector"))
predictNaive=data.frame(predict(fit_naive, newdata=test, type="raw"))
for(prior in x_vector){
treeClass = class(predictTree$good, 'good', 'bad', prior)
treeConfusion=table(test$good_bad, treeClass)
if(ncol(treeConfusion)==1){
if(colnames(treeConfusion)=="good"){
treeConfusion=cbind(c(0,0), treeConfusion)
} else {
treeConfusion=cbind(treeConfusion,c(0,0))
}
}
totGood=sum(treeConfusion[2,])
totBad=sum(treeConfusion[1,])
tpr_tree=c(tpr_tree, treeConfusion[2,2]/totGood)
fpr_tree=c(fpr_tree, treeConfusion[1,2]/totBad)
naiveClass=class(predictNaive$good, 'good', 'bad', prior)
naiveConfusion=table(test$good_bad, naiveClass)
if(ncol(naiveConfusion)==1){
if(colnames(naiveConfusion)=="good"){
naiveConfusion=cbind(c(0,0), naiveConfusion)
} else {
naiveConfusion=cbind(naiveConfusion,c(0,0))
}
}
totGood=sum(naiveConfusion[2,])
totBad=sum(naiveConfusion[1,])
tpr_naive=c(tpr_naive, naiveConfusion[2,2]/totGood)
fpr_naive=c(fpr_naive, naiveConfusion[1,2]/totBad)
}
#Plot the ROC curves
plot(fpr_naive, tpr_naive, main="ROC curve", sub="Red = Naive Bayes, Blue = Tree",
type="l", col="red", xlim=c(0,1), ylim=c(0,1), xlab="FPR", ylab="TPR")
points(fpr_tree, tpr_tree, type="l", col="blue")
#Naive has greatest AOC => should choose Naive
#6: Repeate Naive Bayes with loss matrix punishing with factor 10 if predicting good when
#bad and 1 if predicting bad when good.
naiveModel=naiveBayes(good_bad~., data=train)
train_loss=predict(naiveModel, newdata=train, type="raw")
test_loss=predict(naiveModel, newdata=test, type="raw")
confusion_trainLoss=table(train$good_bad, ifelse(train_loss[,2]/train_loss[,1]>10, "good",
"bad"))
misclass_trainLoss=misclass(confusion_trainLoss, train)
print(confusion_trainLoss)
print(misclass_trainLoss)
confusion_testLoss=table(test$good_bad, ifelse(test_loss[,2]/test_loss[,1]>10, "good",
"bad"))
misclass_testLoss=misclass(confusion_testLoss, test)
print(confusion_testLoss)
print(misclass_testLoss)
#1: Read data
data=read.csv2("NIRspectra.csv")
data$Viscosity=c()
n=dim(data)[1]
#1: Conduct standard PCA using the feature space and provide a plot explaining how much
#variation is explained by each feature. Provide plot that show the scores of PC1 vs PC2.
#Are there unusual diesel fuels according to this plot.
pcaAnalysis=prcomp(data)
#Eigenvalues
lambda=pcaAnalysis$sdev^2
#Proportion of variation
propVar= lambda/sum(lambda)*100
screeplot(pcaAnalysis, main="Total variation from PCA components")
noOfVars=1
sumOfVariation=propVar[noOfVars]
while(sumOfVariation<99){
noOfVars=noOfVars+1
sumOfVariation=sumOfVariation+propVar[noOfVars]
}
#Print number of variables used and total variation
print(noOfVars)
print(sumOfVariation)
#Print PC1 and PC2 in plot
plot(pcaAnalysis$x[,1],pcaAnalysis$x[,2], type="p", col="blue", main="PC1 vs PC2",
xlab="PC1", ylab="PC2")
#We can see from the graph that the data is very accurately described by PC1.
#2: Make trace plots of the loadings of the components selected in step 1. Is there any
#principle component that is explaines by mainly a few original features?
U=pcaAnalysis$rotation
plot(U[,1], main="Traceplot, PC1", xlab="index", ylab="PC1", type="b")
plot(U[,2], main="Traceplot, PC2", xlab="index", ylab="PC2", type="b")
#We can see from graph that PC2 is not described by so many original features since it is
#close to zero for many of the features. The last 30 or so variables have an effect on PC2.
#3: Perform independent Component Analysis (ICA) with no of components selected in step1
#(set seed 12345). Check the documentation of R for fastICA method and do following:
# Compute W'=K*W and present columns of W' in form of the trace plots. Compare with trace
# plots in step 2 and make conclusions. What kind of measure is represented by the matrix W'.
# Make a plot of the scores of the first two latent features and compare it with the score
# plot from step 1.
#Install package fastICa
#install.packages("fastICA")
library("fastICA")
set.seed(12345)
icaModel = fastICA(data, n.comp=2, verbose=TRUE)
W=icaModel$W
K=icaModel$K
W_est=K%*%W
plot(W_est[,1], main="Traceplot, ICA1", xlab="index", ylab="ICA1", type="b", col="red")
plot(W_est[,2], main="Traceplot, ICA2", xlab="index", ylab="ICA2", type="b", col="red")
#Compared to the plots in step 2 the ICA1 follows in roughly the same pattern as PCA2
#and ICA2 the same as PCA1.
plot(icaModel$S[,1], icaModel$S[,2], main="ICA1 vs ICA2", xlab="ICA1", ylab="ICA2",
type="p", col="blue")
#We can see from the plot that the dat is pretty well described by ICA2 whereas ICA1 is
#not that significant in describing the data (since it is close to 0 most of the cases).
#Some outliers are however described by ICA1.
setwd("~/SKOLA/LIU/Åk 4/TDDE01/TDDE01_Labs/Lab3 Block 1")
---
title: "Lab3"
author: "Christian von Koch, Alice Velander, William Anzén"
date: '2019-12-17'
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
RNGversion('3.5.1')
```
## Assignment 1
```{r}
set.seed(1234567890)
#install.packages("geosphere")
library(geosphere)
stations <- read.csv("stations.csv")
temps <- read.csv("temps50k.csv")
#A join operation on "station_number"
st <- merge(stations,temps,by="station_number")
n = dim(st)[1]
#Kernel weighting factors
h_distance <- 100000
h_date <- 20
h_time <- 2
#Latitude of interest
a <- 59.4059
#Longitude of interest
b <- 18.0256
#Coordinates for Danderyd
#Create a vector of the point of interest
placeOI = c(a, b)
dateOI <- as.Date("1995-07-29") # The date to predict (up to the students), my birth date
timesOI = c("04:00:00", "06:00:00", "08:00:00", "10:00:00", "12:00:00", "14:00:00", "16:00:00", "18:00:00", "20:00:00",
"22:00:00", "24:00:00")
```
The code above reads the relevant data set and sets the initial values. The h values have been determined from the graphs shown below. The location of interest chosen is Danderyds kommun in Sweden, and the date of interest is the 29th of July 1995.
```{r}
plotDist = function(dist, h){
u = dist/h
plot(dist, exp(-u^2), type="l", main="Plot of kernel wights for distances", xlab="Distance")
}
dist = seq(0, 100000, 1)
plotDist(dist, h_distance)
plotDate = function(date, h){
u = date/h
plot(date, exp(-u^2), type="l", main="Plot of kernel wights for dates", xlab="Days")
}
date = seq(-182,182,1)
plotDate(date, h_date)
plotTime = function(time, h){
u = time/h
plot(time, exp(-u^2), type="l", main="Plot of kernel wights for time", xlab="Hours")
}
time = seq(-12,12,1)
plotTime(time, h_time)
```
When studying the graphs above further, the h values can be motivated. For distance, it is noteable that the kernel does not give weight to measurements with a distance of 100 000 metres from the location of interest. It gives quite a little weight to distances which are around 50 000 metres from the location of interest. This seems reasonable since not a lot of data points are at the exact place of the location of interest and therefore the function needs to account for measurements which is further away to be able to draw a conclusion regarding the temperature. When the difference in distance is further than 100 000 metres it also seems reasonable to not account for that measurement when predicting the temperature. Elaborating further regarding the date, it again seems reasonable to apply an h value which does not give weight to a date difference which is more than around 30 days. Since the time of year (especially in Sweden) is changing quite fast the function should not account for temperature measurements at dates more than around a month from the date of interest. Similarily with time, the function should not account for time differences of more than 5 hours since the temperature can change significantly during this time.
```{r}
#Remove posterior data
filter_posterior = function(date, time, data){
return(data[which(as.numeric(difftime(strptime(paste(date, time, sep=" "), format="%Y-%m-%d %H:%M:%S"),
strptime(paste(data$date, data$time, sep=" "),format="%Y-%m-%d %H:%M:%S")))>0), ])
}
#A gaussian function for the difference in distance
gaussian_dist = function(place, data, h) {
lat = data$latitude
long = data$longitude
points = data.frame(lat,long)
u = distHaversine(points, place)/h
return (exp(-u^2))
}
xy = gaussian_dist(placeOI, st, h_distance)
#A gaussian function for difference in days
gaussian_day = function(date, data, h){
compare_date = as.Date(data$date)
diff = as.numeric(date-compare_date)
for (i in 1:length(diff)) {
if (diff[i] > 365) {
diff[i] = diff[i] %% 365
if(diff[i]>182){
diff[i]=365-diff[i]
}
}
}
u = diff/h
return (exp(-u^2))
}
#A gaussian function for difference in hours
gaussian_hour = function(hour, data, h){
compare_hour = strptime(data$time, format="%H:%M:%S")
compare_hour = as.numeric(format(compare_hour, format="%H"))
hour = strptime(hour, format="%H:%M:%S")
hour = as.numeric(format(hour, format="%H"))
diff = abs(hour-compare_hour)
for (i in 1:length(diff)){
if(diff[i]>12){
diff[i] = 24-diff[i]
}
}
u=diff/h
return(exp(-u^2))
}
```
The functions above are used to filter out relevant data points that will be used for the temperature estimation of a specific date and time as well as computing the different kernel functions for differences of distance, date and time between the values in the data and the location and time of interest.
```{r}
#Defining values that will be used in loop below
kernel_sum = c()
kernel_mult = c()
#Looping through time array and data points in nested loop to calculate the 11 kernel values
for (time in timesOI) {
filtered_data = filter_posterior(dateOI, time, st)
kernel_dist = gaussian_dist(placeOI, filtered_data, h_distance)
kernel_day = gaussian_day(dateOI, filtered_data, h_date)
kernel_time = gaussian_hour(time, filtered_data, h_time)
sum_kernel = kernel_dist+kernel_day+kernel_time
temp_sum = sum(sum_kernel * filtered_data$air_temperature)/sum(sum_kernel)
mult_kernel = kernel_dist*kernel_day*kernel_time
temp_mult = sum(mult_kernel * filtered_data$air_temperature)/sum(mult_kernel)
kernel_sum = c(kernel_sum, temp_sum)
kernel_mult = c(kernel_mult, temp_mult)
}
plot(kernel_sum, type="o", main ="Temperature estimate through sum of factors", xlab="Time",
ylab="Est. temperature")
axis(1, at=1:length(timesOI), labels=timesOI)
plot(kernel_mult, type="o", main="Temperature estimate through product of factors", xlab="Time",
ylab="Est. temperature")
axis(1, at=1:length(timesOI), labels=(timesOI))
```
Finally, the estimations for the temperatures are made through the summation of the different kernel functions as well as multiplication of the differnt kernel functions. It is noteable that the curves for the summation of kernel functions differs from the multiplication of kernel functions. The summation of kernel functions has provided a lower temperature estimate than what the multiplication of kernel functions has provided. This can be due to that data points which have received a high weight through the kernel functions will have more impact in the multiplication of kernel functions than with the summation of kernel functions, and similarily data points which have received a low weight through the kernel functions will have more impact in the multiplication of kernel functions than with the summation of kernel functions. The result of this might be that the summation of kernel values provides an estimate of the temperature which is more similar to the mean of all the temperatures in the dataset than what the multiplication of kernel values provides.
## Assignment 3
```{r}
#install.packages("neuralnet")
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
train <- trva[1:25,] # Training
valid <- trva[26:50,] # Validation
n = dim(valid)[1]
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(31, -1, 1)
trainScore = rep(0,10)
validScore = rep(0,10)
```
The code above, initializes relevant values and creates the data frame for random numbers between 0 and 10 which in turn are applied to the sinus function. The data is divided into training data and validation data in an 1:1 relation. The weights are initialized randomely. 31 weights are initialized due to a hidden layer of 10 neural nodes, 1 input node and 1 output node which give 10\*1 (from input to hidden layer) + 10\*1 (from hidden layer to output) + 1\*10 (from input bias to hidden layer) + 1\*1 (from hidden layer bias to output) = 31 arrows, i.e. weights.
```{r}
for(i in 1:10) {
nn_temp <- neuralnet(Sin~Var, data=train, hidden=10, threshold=i/1000, startweights=winit)
nn = as.data.frame(nn_temp$net.result)
pred=predict(nn_temp, newdata=valid)
trainScore[i] = 1/n*sum((nn[,1]-train$Sin)^2)
validScore[i] = 1/n*sum((pred-valid$Sin)^2)
}
plot(1:10, trainScore[1:10], main="Plot of MSE on train and valid data depending on threshold value", type="b", col="red", xlab="Threshold index", ylab="MSE", sub="Blue = valid data, Red = train data")
points(1:10, validScore[1:10], type="b", col="blue")
min_error=min(validScore[1:10])
print(min_error)
optimal_i=which(validScore[1:10] == min_error)
print(optimal_i)
```
As seen in the graph above, naturally the train data performs the best when the threshold value is as small as possible, i.e. 1/1000, and performance decreases as this threshold increases for the train data. From the graph we can see that the threshold value 4/1000 performs the best on the validation data (since it results in the lowest MSE) and therefore this threshold will be used moving forward in the assignment.
```{r}
optimal_nn = neuralnet(Sin~Var, data=train, hidden=10, threshold=optimal_i/1000, startweights=winit)
plot(optimal_nn, fontsize = 10, rep="best")
# Plot of the predictions (black dots) and the data (red dots)
plot(prediction(optimal_nn)$rep1, xlim=c(0,10), ylim=c(-1,1), main = "Plot of neural predicted values", sub="Black = predictions, Red = data")
points(trva, col = "red")
```
The optimal neural network with threshold 4/1000 is chosen which results in the neural network shown above. The last two graphs briefly show how similar the predicted values from the model are in comparison to the real sinus curve. One can conclude that the neural network created resembles the shape of the sinus curve with quite a precision.
\newpage
## Appendix
### Code for Assignment 1
```{r eval = FALSE}
RNGversion('3.5.1')
set.seed(1234567890)
#install.packages("geosphere")
library(geosphere)
stations <- read.csv("stations.csv")
temps <- read.csv("temps50k.csv")
#A join operation on "station_number"
st <- merge(stations,temps,by="station_number")
n = dim(st)[1]
#Kernel weighting factors
h_distance <- 100000
h_date <- 20
h_time <- 2
#Latitude of interest
a <- 59.4059
#Longitude of interest
b <- 18.0256
#Coordinates for Danderyd
#Create a vector of the point of interest
placeOI = c(a, b)
dateOI <- as.Date("1995-07-29") # The date to predict (up to the students), my birth date
timesOI = c("04:00:00", "06:00:00", "08:00:00", "10:00:00", "12:00:00", "14:00:00", "16:00:00", "18:00:00", "20:00:00",
"22:00:00", "24:00:00")
plotDist = function(dist, h){
u = dist/h
plot(exp(-u^2), type="l", main="Plot of kernel wights for distances", xlab="Distance")
}
dist = seq(0, 100000, 1)
plotDist(dist, h_distance)
plotDate = function(date, h){
u = date/h
plot(exp(-u^2), type="l", main="Plot of kernel wights for dates", xlab="Days")
}
date = seq(-182,182,1)
plotDate(date, h_date)
plotTime = function(time, h){
u = time/h
plot(exp(-u^2), type="l", main="Plot of kernel wights for time", xlab="Hours")
}
time = seq(-12,12,1)
plotTime(time, h_time)
#Remove posterior data
filter_posterior = function(date, time, data){
return(data[which(as.numeric(difftime(strptime(paste(date, time, sep=" "), format="%Y-%m-%d %H:%M:%S"),
strptime(paste(data$date, data$time, sep=" "),format="%Y-%m-%d %H:%M:%S")))>0), ])
}
#A gaussian function for the difference in distance
gaussian_dist = function(place, data, h) {
lat = data$latitude
long = data$longitude
points = data.frame(lat,long)
u = distHaversine(points, place)/h
return (exp(-u^2))
}
xy = gaussian_dist(placeOI, st, h_distance)
#A gaussian function for difference in days
gaussian_day = function(date, data, h){
compare_date = as.Date(data$date)
diff = as.numeric(date-compare_date)
for (i in 1:length(diff)) {
if (diff[i] > 365) {
diff[i] = diff[i] %% 365
if(diff[i]>182){
diff[i]=365-diff[i]
}
}
}
u = diff/h
return (exp(-u^2))
}
#A gaussian function for difference in hours
gaussian_hour = function(hour, data, h){
compare_hour = strptime(data$time, format="%H:%M:%S")
compare_hour = as.numeric(format(compare_hour, format="%H"))
hour = strptime(hour, format="%H:%M:%S")
hour = as.numeric(format(hour, format="%H"))
diff = abs(hour-compare_hour)
for (i in 1:length(diff)){
if(diff[i]>12){
diff[i] = 24-diff[i]
}
}
u=diff/h
return(exp(-u^2))
}
#Defining values that will be used in loop below
kernel_sum = c()
kernel_mult = c()
#Looping through time array and data points in nested loop to calculate the 11 kernel values
for (time in timesOI) {
filtered_data = filter_posterior(dateOI, time, st)
kernel_dist = gaussian_dist(placeOI, filtered_data, h_distance)
kernel_day = gaussian_day(dateOI, filtered_data, h_date)
kernel_time = gaussian_hour(time, filtered_data, h_time)
sum_kernel = kernel_dist+kernel_day+kernel_time
temp_sum = sum(sum_kernel * filtered_data$air_temperature)/sum(sum_kernel)
mult_kernel = kernel_dist*kernel_day*kernel_time
temp_mult = sum(mult_kernel * filtered_data$air_temperature)/sum(mult_kernel)
kernel_sum = c(kernel_sum, temp_sum)
kernel_mult = c(kernel_mult, temp_mult)
}
plot(kernel_sum, type="o", main ="Temperature estimate through sum of factors", xlab="Time",
ylab="Est. temperature")
axis(1, at=1:length(timesOI), labels=timesOI)
plot(kernel_mult, type="o", main="Temperature estimate through product of factors", xlab="Time",
ylab="Est. temperature")
axis(1, at=1:length(timesOI), labels=(timesOI))
```
### Code for Assignment 3
```{r eval = FALSE}
RNGversion('3.5.1')
#install.packages("neuralnet")
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
train <- trva[1:25,] # Training
valid <- trva[26:50,] # Validation
n = dim(valid)[1]
# Random initialization of the weights in the interval [-1, 1]
winit <- runif(31, -1, 1)
trainScore = rep(0,10)
validScore = rep(0,10)
for(i in 1:10) {
nn_temp <- neuralnet(Sin~Var, data=train, hidden=10, threshold=i/1000, startweights=winit)
nn = as.data.frame(nn_temp$net.result)
pred=predict(nn_temp, newdata=valid)
trainScore[i] = 1/n*sum((nn[,1]-train$Sin)^2)
validScore[i] = 1/n*sum((pred-valid$Sin)^2)
}
plot(1:10, trainScore[1:10], type="b", col="red", xlab="Threshold index", ylab="MSE")
points(1:10, validScore[1:10], type="b", col="blue")
min_error=min(validScore[1:10])
print(min_error)
optimal_i=which(validScore[1:10] == min_error)
print(optimal_i)
optimal_nn = neuralnet(Sin~Var, data=train, hidden=10, threshold=optimal_i/1000, startweights=winit)
plot(optimal_nn)
# Plot of the predictions (black dots) and the data (red dots)
par(new=FALSE)
plot(prediction(optimal_nn)$rep1)
points(trva, col = "red")
mean(st$air_temperature)
source('~/SKOLA/LIU/Åk 4/TDDE01/TDDE01_Labs/Lab3 Block 1/Lab3_Assignment1.R', echo=TRUE)
mean(st$air_temperature)
source('~/SKOLA/LIU/Åk 4/TDDE01/TDDE01_Labs/Lab3 Block 1/Lab3_Assignment1.R', echo=TRUE)
source('~/SKOLA/LIU/Åk 4/TDDE01/TDDE01_Labs/Lab3 Block 1/Lab3_Assignment1.R', echo=TRUE)
